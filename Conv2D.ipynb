{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimdyoc13/Cheese_house/blob/main/Conv2D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1xjKgidmxTR",
        "outputId": "38ff560f-27d9-4876-d1b4-a326cdab1b40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bskm3X5e3yW3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhgirIvj3yXB",
        "outputId": "7d94e174-97c0-4aa0-8e67-ade0426e841a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이미지 크기: 1920 x 1080 픽셀\n",
            "이미지 모드: RGB\n",
            "이미지 형식: JPEG\n",
            "이미지 정보:\n",
            "jfif: 257\n",
            "jfif_version: (1, 1)\n",
            "jfif_unit: 0\n",
            "jfif_density: (1, 1)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# 이미지 파일 경로\n",
        "image_path = \"/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1.jpg\"\n",
        "\n",
        "# 이미지 열기\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# 이미지의 크기 (픽셀)\n",
        "width, height = image.size\n",
        "print(f\"이미지 크기: {width} x {height} 픽셀\")\n",
        "\n",
        "# 이미지 모드 (예: RGB, 그레이스케일)\n",
        "image_mode = image.mode\n",
        "print(f\"이미지 모드: {image_mode}\")\n",
        "\n",
        "# 이미지 파일 형식 (예: JPEG, PNG)\n",
        "image_format = image.format\n",
        "print(f\"이미지 형식: {image_format}\")\n",
        "\n",
        "# 이미지 정보 출력\n",
        "image_info = image.info\n",
        "print(\"이미지 정보:\")\n",
        "for key, value in image_info.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "# 이미지 표시 (선택 사항)\n",
        "image.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GrZEudsA3yW7"
      },
      "outputs": [],
      "source": [
        "# 경로 설정\n",
        "train_path = '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/train'\n",
        "test_path = '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDgZDok13yXA",
        "outputId": "937a9266-cb9c-414a-dac9-0e40db1a7b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1814.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/271.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/44.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/487.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/924.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/942.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1233.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1349.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1377.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1388.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1564.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1814.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/271.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/44.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/487.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/924.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/942.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1233.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1349.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1377.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1388.jpg\n",
            "손상된 이미지: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1564.jpg\n"
          ]
        }
      ],
      "source": [
        "# 손상된 이미지 검출\n",
        "import os\n",
        "from PIL import Image\n",
        "from PIL import UnidentifiedImageError  # Python 3.6 이상에서 사용 가능\n",
        "\n",
        "def find_corrupted_images(directory):\n",
        "    corrupted_images = []\n",
        "\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
        "                image_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    # 이미지를 열어보고 예외가 발생하는 경우 손상된 이미지로 처리\n",
        "                    with Image.open(image_path) as img:\n",
        "                        img.load()\n",
        "                except (UnidentifiedImageError, OSError):\n",
        "                    print(f\"손상된 이미지: {image_path}\")\n",
        "                    corrupted_images.append(image_path)\n",
        "\n",
        "    return corrupted_images\n",
        "\n",
        "# 이미지 파일을 검색할 디렉토리 지정\n",
        "image_directory = '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test'\n",
        "\n",
        "# 손상된 이미지 찾기\n",
        "corrupted_images = find_corrupted_images(image_directory)\n",
        "\n",
        "# 찾은 손상된 이미지 출력\n",
        "for corrupted_image in corrupted_images:\n",
        "    print(f\"손상된 이미지: {corrupted_image}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyWrny_o3yXB",
        "outputId": "269dfbdb-e4b2-4e5a-c9e0-d7d437f5044e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1814.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/271.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/44.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/487.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/924.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/942.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1233.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1349.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1377.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1388.jpg\n",
            "대체 완료: /content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1564.jpg\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 손상된 이미지 파일 경로\n",
        "corrupted_image_paths = [\n",
        "'/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1814.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/271.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/44.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/487.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/924.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/942.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1233.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1349.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1377.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1388.jpg'\n",
        ", '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test/1564.jpg'\n",
        "]\n",
        "\n",
        "# 대체할 이미지 크기와 색상 설정 (하얀색)\n",
        "width, height = 1920, 1080\n",
        "white_color = (255, 255, 255)\n",
        "\n",
        "# 손상된 이미지를 하얀색 이미지로 대체하고 저장\n",
        "for image_path in corrupted_image_paths:\n",
        "    try:\n",
        "        # 손상된 이미지를 엽니다.\n",
        "        corrupted_image = Image.open(image_path)\n",
        "\n",
        "        # 손상된 이미지를 하얀색 이미지로 대체합니다.\n",
        "        white_image = Image.new(\"RGB\", (width, height), white_color)\n",
        "\n",
        "        # 대체된 이미지를 저장합니다.\n",
        "        white_image.save(image_path)\n",
        "\n",
        "        print(f\"대체 완료: {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"대체 실패: {image_path}, 에러: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMyOFwHd3yW8",
        "outputId": "6cb73e42-1b99-4140-e183-d0b3f50cfd1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4454 files belonging to 2 classes.\n",
            "Using 3564 files for training.\n"
          ]
        }
      ],
      "source": [
        "# 트레이닝 데이터 로드 및 전처리\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    directory=train_path,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123  # 재현성을 위한 시드\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHmGnwSb3yW9",
        "outputId": "345630c3-5833-4448-826a-e14de2a3aaec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4454 files belonging to 2 classes.\n",
            "Using 890 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# 검증 데이터 로드 및 전처리\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    directory=train_path,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",  # validation 데이터 선택\n",
        "    seed=123  # 재현성을 위한 시드\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ynmZgZ3yW9",
        "outputId": "c61a8a50-ba7c-4699-925a-08f7aefada4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1828 files belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 테스트 데이터 로드 및 전처리\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    directory=test_path,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    shuffle=False,  # 순서를 고정\n",
        "    labels=None,  # 라벨이 없음\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2ML9vUZI3yW-"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.MaxPool2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(2, 2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raG-jT5V3yW-",
        "outputId": "6d082605-5c9b-401b-c502-a0d95da51f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "112/112 [==============================] - 96s 104ms/step - loss: 0.0129 - accuracy: 0.9944 - val_loss: 1.5157e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "112/112 [==============================] - 16s 135ms/step - loss: 1.1369e-05 - accuracy: 1.0000 - val_loss: 7.0322e-08 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "112/112 [==============================] - 12s 106ms/step - loss: 2.3975e-06 - accuracy: 1.0000 - val_loss: 1.3923e-06 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "112/112 [==============================] - 13s 111ms/step - loss: 2.8038e-06 - accuracy: 1.0000 - val_loss: 2.7164e-06 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "112/112 [==============================] - 13s 112ms/step - loss: 1.6210e-06 - accuracy: 1.0000 - val_loss: 4.1046e-06 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78ed5c077580>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 학습\n",
        "model.fit(train_dataset, epochs=5, validation_data=validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhdXUlID3yXA",
        "outputId": "40eb4eb4-760b-4757-801f-cf189d588706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이미지 파일이 손상되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# # 데이터 무결성 평가\n",
        "\n",
        "# import os\n",
        "\n",
        "# def is_image_corrupted(file_path):\n",
        "#     try:\n",
        "#         # 파일 크기 확인\n",
        "#         file_size = os.path.getsize(file_path)\n",
        "#         if file_size < 1:\n",
        "#             return True  # 파일 크기가 0 또는 음수인 경우 손상된 것으로 간주\n",
        "\n",
        "#         # 파일 확장자 확인 (예: .jpg, .png, .bmp 등)\n",
        "#         file_extension = os.path.splitext(file_path)[1].lower()\n",
        "#         if file_extension not in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "#             return True  # 지원하지 않는 이미지 확장자인 경우\n",
        "\n",
        "#         # 파일을 이미지로 열어보고 검증\n",
        "#         with open(file_path, 'rb') as image_file:\n",
        "#             image_file.read()  # 이미지 파일을 읽어보고 예외가 발생하면 손상된 것으로 간주\n",
        "#         return False  # 이미지 파일은 정상\n",
        "\n",
        "#     except Exception as e:\n",
        "#         return True  # 다른 예외가 발생한 경우 손상된 것으로 간주\n",
        "\n",
        "# # 이미지 파일 경로\n",
        "# image_file_path = '/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/test'\n",
        "\n",
        "# if is_image_corrupted(image_file_path):\n",
        "#     print(\"이미지 파일이 손상되었습니다.\")\n",
        "# else:\n",
        "#     print(\"이미지 파일은 정상입니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "j6Z-L9oP3yXA",
        "outputId": "48793604-c863-4761-b91c-ef3030aa99c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 8/58 [===>..........................] - ETA: 17s"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-38270053654f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 평가 및 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 결과를 이진 형태로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbinary_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2631\u001b[0;31m                         \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m                             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    874\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    877\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 모델 평가 및 예측\n",
        "predictions = model.predict(test_dataset)\n",
        "\n",
        "# 결과를 이진 형태로 변환\n",
        "#반올림\n",
        "binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\n",
        "\n",
        "# 결과를 CSV로 저장\n",
        "results_df = pd.DataFrame({'Id': list(range(1, len(binary_predictions) + 1)),\n",
        "                           'Class': binary_predictions})\n",
        "results_df.to_csv('/content/drive/MyDrive/School/capstone/2023-uou-hd-dt-2-cv/Submission/result_conv2D.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwFpO_yM3yXB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}